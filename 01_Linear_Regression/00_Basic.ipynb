{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    1. Find SSE without applying OLS.\n",
    "    2. Find SSE after applying OSL and compare with above.\n",
    "       (Solve for alpha and beta).\n",
    "    4. Find SSE of scikit model and compare your results achieved by manual calculation.\n",
    "    5. Multivariable linear regression using scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Square method/Simple Linear Regression\n",
    "### (Single explanatory variable and single response variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squared error before applying ordinary least square:  92.8\n"
     ]
    }
   ],
   "source": [
    "# What is Sum of Squared Error (SSE) before applying ordinary least square \n",
    "x_train = [6, 8, 10, 14, 18]\n",
    "y_train = [7, 9, 13, 17, 18] \n",
    "\n",
    "y_train_avg = (7 + 9 + 13 + 17 + 18) / 5\n",
    "sse_1 = pow((7-y_train_avg),2)+pow((9-y_train_avg),2)+pow((13-y_train_avg),2)+pow((17-y_train_avg),2)+pow((18-y_train_avg),2)\n",
    "print(\"Sum of Squared error before applying ordinary least square: \",sse_1)\n",
    "\n",
    "#To do the best fit of line intercept, apply a linear regression model to reduce the SSE value at\n",
    "#minimum as possible."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For simple linear regression (also called ordinary least square linear reg):\n",
    "                y = α + β x\n",
    "The intercept term α and coefficient β are parameters of the model that are learned by the learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving  α and  β "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "simple linear regression is given by the following equation (algebric notation):\n",
    "            y = α + β x\n",
    "where,\n",
    "    y is hypothesis or predicted output (y is scalar variable as assumed algebric notation)\n",
    "    x is explanatory variable in training data set (x scalar variable variable as assumed algebric notation)\n",
    "    α coefficient \n",
    "    β coefficient "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Solve β first. To do so, calculate the variance of x and  covariance of x and y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Variance is a measure of how far a set of values is spread out. If all of the numbers\n",
    "in the set are equal, the variance of the set is zero. A small variance indicates that the\n",
    "numbers are near the mean of the set, while a set containing numbers that are far\n",
    "from the mean and each other will have a large variance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Covariance is a measure of how much two variables change together. If the value of\n",
    "the variables increase together, their covariance is positive. If one variable tends to\n",
    "increase while the other decreases, their covariance is negative. If there is no linear\n",
    "relationship between the two variables, their covariance will be equal to zero; the\n",
    "variables are linearly uncorrelated but not necessarily independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbar = (6 + 8 + 10 + 14 + 18) / 5  # mean of x\n",
    "ybar = (7 + 9 + 13 + 17 + 18) / 5  # mean of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#NumPy provides the var method to calculate variance\n",
    "np.var([6, 8, 10, 14, 18], ddof=1)\n",
    "#NumPy provides the cov method to calculate co-variance\n",
    "np.cov([6, 8, 10, 14, 18], [7, 9, 13, 17, 18])[0][1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "With variance of explanatory variable and the covariance of the response and explanatory variables, we can solve β using the following formula:\n",
    "β = cov ( x , y )/var ( x )\n",
    "β = 22.65/23.2 = 0.9762931034482758"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Having β , we can solve α using the following formula:\n",
    "    α = (ybar − β.xbar)\n",
    "    α = ( ybar− β.xbar)\n",
    "    α = (12.9 - (0.92*11.2))\n",
    "    α = 1.9655172413793114"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So coefficients are α=1.96 and β=0.97."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having alpha and beta, calculate prediction using algebric notation: y = α + β x\n",
    "# substitu explanatory varialbe value, alpha, beta in equation y = α + β x for each prediction\n",
    "# example:\n",
    "y1 = 1.9655172413793114 + (0.9762931034482758 * 6) #for i=1\n",
    "y2 = 1.9655172413793114 + (0.9762931034482758 * 8) #for 1=2\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**************************************** MATRIX FORM *************************************************\n",
    "The linear regression model can be written in matrix form as:\n",
    "    Y=X.C\n",
    "where Y is a (T×1) vector, X is a (T×K) matrix, C is a (K×1) vector\n",
    "Note the distinction! X and C are matrices not like a and  x."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In order for two matrices to be conformable for matrix multiplication, the number of columns of the left matrix must be the same as the number of rows of the right matrix.\n",
    "In the linear regression model, Xβ is possible because X, the left matrix, has K columns and β, the right matrix, has K rows.\n",
    "On the other hand, βX would not be possible because β, the first matrix, has 1 column while X, the second matrix, has T rows - unless, of course, T=1.\n",
    "Hence to make matrix multiplication comfortable insert element 1 to each value of x anf form a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.82327586],\n",
       "       [ 9.77586207],\n",
       "       [11.72844828],\n",
       "       [15.63362069],\n",
       "       [19.5387931 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having alpha and beta, calculate prediction using matrix notation\n",
    "matrix_x = [[1,6], [1,8], [1,10], [1,14],[1,18]] #this is explatory variable matrix having X variables\n",
    "coeff = [[1.9655172413793114],[0.9762931034482758]] #this is coefficient matrix having [alpha,beta] \n",
    "prediction_on_x_train = np.dot(matrix_x, coeff) #matrix_X.coeff  dot product\n",
    "prediction_on_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.823275862068966, 9.775862068965518, 11.72844827586207, 15.633620689655173, 19.538793103448278]\n",
      "\n",
      "Sum of Squared error after applying ordinary least square:  7.131465517241385\n"
     ]
    }
   ],
   "source": [
    "# What is Sum of Squared Error (SSE) after applying ordinary least square Linear Reg on x_train data\n",
    "y_train = [7, 9, 13, 17, 18] \n",
    "prediction_on_x_train_list=[]\n",
    "\n",
    "size = prediction_on_x_train.shape[0]\n",
    "for i in range (size):\n",
    "    m_list = prediction_on_x_train.tolist()[i]\n",
    "    prediction_on_x_train_list.insert(i, m_list[0])\n",
    "\n",
    "print(prediction_on_x_train_list) \n",
    "\n",
    "sse_2 = pow((7-prediction_on_x_train_list[0]),2)+pow((9-prediction_on_x_train_list[1]),2)+pow((13-prediction_on_x_train_list[2]),2)+pow((17-prediction_on_x_train_list[3]),2)+pow((18-prediction_on_x_train_list[4]),2)   \n",
    "\n",
    "print(\"\\nSum of Squared error after applying ordinary least square: \",sse_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above intuition can be done using scikit learn linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.80172414],\n",
       "       [ 9.72413793],\n",
       "       [11.64655172],\n",
       "       [15.49137931],\n",
       "       [19.3362069 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Data set\n",
    "x_train = [[6], [8], [10], [14],[18]] # vector form\n",
    "#x_test = [[12], [7], [9], [26],[17]]\n",
    "y_train = [[7], [9], [13], [17], [18]] \n",
    "# Create model\n",
    "model = LinearRegression()\n",
    "# Fit model\n",
    "model.fit(x_train,y_train)\n",
    "# Predict price\n",
    "predicted_on_x_train = model.predict(x_train)\n",
    "#predicted_price_on_x_test = model.predict(x_test)\n",
    "predicted_on_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.8017241379310365, 9.724137931034484, 11.646551724137932, 15.49137931034483, 19.336206896551722]\n",
      "\n",
      "Sum of Squared error after applying scikit learn linear reg:  7.060344827586198\n"
     ]
    }
   ],
   "source": [
    "y_train = [7, 9, 13, 17, 18] \n",
    "predicted_on_x_train_list=[]\n",
    "\n",
    "size = predicted_on_x_train.shape[0]\n",
    "for i in range (size):\n",
    "    m_list = predicted_on_x_train.tolist()[i]\n",
    "    predicted_on_x_train_list.insert(i, m_list[0])\n",
    "\n",
    "print(predicted_on_x_train_list) \n",
    "\n",
    "sse_2 = pow((7-predicted_on_x_train_list[0]),2)+pow((9-predicted_on_x_train_list[1]),2)+pow((13-predicted_on_x_train_list[2]),2)+pow((17-predicted_on_x_train_list[3]),2)+pow((18-predicted_on_x_train_list[4]),2)   \n",
    "\n",
    "print(\"\\nSum of Squared error after applying scikit learn linear reg: \",sse_2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SSE calculated for ordinary least square is approximatly equal to the SSE calculated for scikit learn\n",
    "model. The terms Ordinay Least Squared and simple linear regression can be used interchangeably for single\n",
    "explanatory variable. More specifically, Ordinay Least Square method is to find alpha and beta for best fitting\n",
    "model (reduce SSE) for simple linear regression having single explanatory variable. To reduce SSE in case of multivarient linear regression 'Gradient descent' is helpful. \n",
    "R-squared is an approach to evaluate model built from ordinary least square method, R-squared tells about accuracy in prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model using r-squared"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Explanatory variable, x = [8,9,11,16,12] \n",
    "Response varible, y= [11, 8.5, 15, 18, 11]\n",
    "Prediction = [9.7759, 10.7522, 12.7048, 17.5863, 13.6811]\n",
    "mean of the response variable, ybar = 12.7"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "r-squared must be a positive number between zero and one.\n",
    "We will follow the method used by scikit-learn to calculate r-squared for our pizza-price predictor."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First, we must measure the total sum of the squares.\n",
    "SStotal = ( 11 − 12.7 )^2 + ( 8.5 − 12.7 )^2 + ⋅⋅⋅ + ( 11 − 12.7 )^2 = 56.8"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, find the residual sum of the squares,this is also cost function.\n",
    "SSres = ( 11 − 9.7759 )^2 + ( 8.5 − 10.7522 )^2 + ⋅⋅⋅ + ( 11 − 13.6811 )^2 = 19.19821359"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally, we can find r-squared using the following formula:\n",
    " R^2 =  1-(SSres/SStotal)\n",
    " R^2 = 1-(19.19/56.8)\n",
    " R^2 = 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating above in sci-kit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = [[6], [8], [10], [14],[18]]\n",
    "y = [[7], [9], [13], [17.5], [18]]\n",
    "X_test = [[8], [9],[11], [16], [12]]\n",
    "y_test = [[11], [8.5], [15], [18], [11]]\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print('R-squared:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression (multiple explanatory variable, single response variable)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train Dataset\n",
    "x1_train = [6,8,10,14,18]\n",
    "x2_train = [2,1,0,2,0]\n",
    "y_train = [7,9,13,17.5,18]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Test Dataset \n",
    "x1_test = [8,9,11,16,12]\n",
    "x2_test = [2,0,2,2,0]\n",
    "y_test = [11,8.5,15,18,11]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Formally, multiple linear regression is the following model:\n",
    "                Y = α + β1x1 + β2x2 + ⋅⋅⋅ + βnxn\n",
    "Where simple linear regression uses a single explanatory variable with a single coefficient, multiple linear regression uses a coefficient for each of an arbitrary number of explanatory variables. In general:\n",
    "                Y = X β    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To find the values of β , which minimize the cost function:\n",
    "                β = (( X^T.X )^(-1)). (X^T.Y)\n",
    "\n",
    "Where, X(combination is X1 and X2) and Y(combination of Y1 and Y2) from training dataset:\n",
    "X = [[1, 6, 2], [1, 8, 1], [1, 10, 0], [1, 14, 2], [1, 18, 0]]\n",
    "Y = [7,9,13,17.5,18]\n",
    "\n",
    "NOTE: β is calculated on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can solve β using NumPy, as follows:\n",
    "from numpy.linalg import inv\n",
    "from numpy import dot, transpose\n",
    "\n",
    "X = [[1, 6, 2], [1, 8, 1], [1, 10, 0], [1, 14, 2], [1, 18, 0]]\n",
    "y = [[7], [9], [13], [17.5], [18]]\n",
    "\n",
    "beta = dot(inv(dot(transpose(X), X)), dot(transpose(X), y))\n",
    "print(beta)\n",
    "beta.shape # beta is 3x1 matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-variable Linear regression using scikit learn model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = [[6, 2], [8, 1], [10, 0], [14, 2], [18, 0]]\n",
    "y = [[7],[9],[13],[17.5], [18]]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "X_test = [[8, 2], [9, 0], [11, 2], [16, 2], [12, 0]]\n",
    "y_test = [[11],[8.5], [15],[18],[11]]\n",
    "predictions = model.predict(X_test)\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print (\"predicted: %d target: %d \" %(prediction[0], y_test[i][0]))\n",
    "\n",
    "#Evaluate model performance\n",
    "print ('R-squared: ', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why transpose"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The commutative property of (matrix) multiplication is not always satisfied in the world of linear algebra;\n",
    "the general case is that it is not satisfied. In other words, in general, Xβ≠βX. Furthermore, such operation\n",
    "may not even be permitted, since in matrix algebra, matrices must be conformable for multiplication if they\n",
    "are to be multiplied.\n",
    "\n",
    "If X is nxm matrix and β is mx1 vector then notation will be like this; without transpose: \n",
    "y=Xβ+ϵ\n",
    "\n",
    "If both X and β are column vectors of dimension (n×1) then perform transform of X.\n",
    "y=β.X^T+ϵ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
