{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#    1. Find SSE without applying OLS.\n",
    "#    2. Find SSE after applying OSL and compare with above.\n",
    "#       (Solve for alpha and beta).\n",
    "#    4. Find SSE of scikit model and compare your results achieved by manual calculation.\n",
    "#    5. Multivariable linear regression using scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Square method/Simple Linear Regression\n",
    "### (Single explanatory variable and single response variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Squared error before applying ordinary least square:  92.8\n"
     ]
    }
   ],
   "source": [
    "# What is Sum of Squared Error (SSE) before applying ordinary least square \n",
    "x_train = [6, 8, 10, 14, 18]\n",
    "y_train = [7, 9, 13, 17, 18] \n",
    "\n",
    "y_train_avg = (7 + 9 + 13 + 17 + 18) / 5\n",
    "sse_1 = pow((7-y_train_avg),2)+pow((9-y_train_avg),2)+pow((13-y_train_avg),2)+pow((17-y_train_avg),2)+pow((18-y_train_avg),2)\n",
    "print(\"Sum of Squared error before applying ordinary least square: \",sse_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving  α and  β "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "simple linear regression (also called ordinary least square linear reg) is given by the following equatio:\n",
    "            y = α + β x\n",
    "where,\n",
    "y is hypothesis or predicted output\n",
    "x is explanatory variable \n",
    "The intercept term α and coefficient β are parameters of the model that are learned by the learning algorithm. \n",
    "Solve β first, to do so calculate the variance of x and  covariance of x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbar = (6 + 8 + 10 + 14 + 18) / 5  # mean of x\n",
    "ybar = (7 + 9 + 13 + 17 + 18) / 5  # mean of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#NumPy provides the var method to calculate variance\n",
    "var = np.var([6, 8, 10, 14, 18], ddof=1)\n",
    "#NumPy provides the cov method to calculate co-variance\n",
    "co_var = np.cov([6, 8, 10, 14, 18], [7, 9, 13, 17, 18])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# β = cov ( x , y )/var ( x )\n",
    "# β = co_var/var = 0.9612068965517242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having β , we can solve α using the following formula:\n",
    "#    α = (ybar − β.xbar)\n",
    "#    α = ( ybar− β.xbar)\n",
    "#    α = 1.9655172413793114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So coefficients are α=1.96 and β=0.97."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having alpha and beta, calculate prediction using algebric notation: y = α + β x\n",
    "# substitu explanatory varialbe value, alpha, beta in equation y = α + β x for each prediction\n",
    "# example:\n",
    "y1 = 1.9655172413793114 + (0.9762931034482758 * 6) #for i=1\n",
    "y2 = 1.9655172413793114 + (0.9762931034482758 * 8) #for 1=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The linear regression model can be written in matrix form as:\n",
    "#    Y=X.C\n",
    "# where Y is a (T×1) vector, X is a (T×K) matrix, C is a (K×1) vector\n",
    "# Note the distinction! X and C are matrices not like a and  x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"In order for two matrices to be conformable for matrix multiplication, the number of columns of the left matrix must be the same as the number of rows of the right matrix.\n",
    "In the linear regression model, Xβ is possible because X, the left matrix, has K columns and β, the right matrix, has K rows.\n",
    "On the other hand, βX would not be possible because β, the first matrix, has 1 column while X, the second matrix, has T rows - unless, of course, T=1.\n",
    "Hence to make matrix multiplication comfortable insert element 1 to each value of x anf form a matrix.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having alpha and beta, calculate prediction using matrix notation\n",
    "matrix_x = [[1,6], [1,8], [1,10], [1,14],[1,18]] #this is explatory variable matrix having X variables\n",
    "coeff = [[1.9655172413793114],[0.9762931034482758]] #this is coefficient matrix having [alpha,beta] \n",
    "prediction_on_x_train = np.dot(matrix_x, coeff) #matrix_X.coeff  dot product\n",
    "#prediction_on_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of Squared error after applying ordinary least square:  7.131465517241385\n"
     ]
    }
   ],
   "source": [
    "# What is Sum of Squared Error (SSE) after applying ordinary least square Linear Reg on x_train data\n",
    "y_train = [7, 9, 13, 17, 18] \n",
    "prediction_on_x_train_list=[]\n",
    "\n",
    "size = prediction_on_x_train.shape[0]\n",
    "for i in range (size):\n",
    "    m_list = prediction_on_x_train.tolist()[i]\n",
    "    prediction_on_x_train_list.insert(i, m_list[0])\n",
    "\n",
    "#print(prediction_on_x_train_list) \n",
    "\n",
    "sse_2 = pow((7-prediction_on_x_train_list[0]),2)+pow((9-prediction_on_x_train_list[1]),2)+pow((13-prediction_on_x_train_list[2]),2)+pow((17-prediction_on_x_train_list[3]),2)+pow((18-prediction_on_x_train_list[4]),2)   \n",
    "\n",
    "print(\"\\nSum of Squared error after applying ordinary least square: \",sse_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above intuition can be done using scikit learn linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Data set\n",
    "x_train = [[6], [8], [10], [14],[18]] # vector form\n",
    "#x_test = [[12], [7], [9], [26],[17]]\n",
    "y_train = [[7], [9], [13], [17], [18]] \n",
    "# Create model\n",
    "model = LinearRegression()\n",
    "# Fit model\n",
    "model.fit(x_train,y_train)\n",
    "# Predict price\n",
    "predicted_on_x_train = model.predict(x_train)\n",
    "#predicted_price_on_x_test = model.predict(x_test)\n",
    "#predicted_on_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of Squared error after applying scikit learn linear reg:  7.060344827586198\n"
     ]
    }
   ],
   "source": [
    "y_train = [7, 9, 13, 17, 18] \n",
    "predicted_on_x_train_list=[]\n",
    "\n",
    "size = predicted_on_x_train.shape[0]\n",
    "for i in range (size):\n",
    "    m_list = predicted_on_x_train.tolist()[i]\n",
    "    predicted_on_x_train_list.insert(i, m_list[0])\n",
    "\n",
    "#print(predicted_on_x_train_list) \n",
    "\n",
    "sse_2 = pow((7-predicted_on_x_train_list[0]),2)+pow((9-predicted_on_x_train_list[1]),2)+pow((13-predicted_on_x_train_list[2]),2)+pow((17-predicted_on_x_train_list[3]),2)+pow((18-predicted_on_x_train_list[4]),2)   \n",
    "\n",
    "print(\"\\nSum of Squared error after applying scikit learn linear reg: \",sse_2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOTE: SSE calculated for ordinary least square is approximatly equal to the SSE calculated for scikit learn\n",
    "model. The terms Ordinay Least Squared and simple linear regression can be used interchangeably for single\n",
    "explanatory variable. More specifically, Ordinay Least Square method is to find alpha and beta for best fitting\n",
    "model (reduce SSE) for simple linear regression having single explanatory variable. To reduce SSE in case of multivarient linear regression 'Gradient descent' is helpful. \n",
    "R-squared is an approach to evaluate model built from ordinary least square method, R-squared tells about accuracy in prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model using r-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanatory variable, x = [8,9,11,16,12] \n",
    "# Response varible, y= [11, 8.5, 15, 18, 11]\n",
    "# Prediction = [9.7759, 10.7522, 12.7048, 17.5863, 13.6811]\n",
    "# mean of the response variable, ybar = 12.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r-squared must be a positive number between zero and one.\n",
    "# We will follow the method used by scikit-learn to calculate r-squared for our pizza-price predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we must measure the total sum of the squares.\n",
    "# SStotal = ( 11 − 12.7 )^2 + ( 8.5 − 12.7 )^2 + ⋅⋅⋅ + ( 11 − 12.7 )^2 = 56.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, find the residual sum of the squares,this is also cost function.\n",
    "# SSres = ( 11 − 9.7759 )^2 + ( 8.5 − 10.7522 )^2 + ⋅⋅⋅ + ( 11 − 13.6811 )^2 = 19.19821359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can find r-squared using the following formula:\n",
    "# R^2 =  1-(SSres/SStotal)\n",
    "# R^2 = 1-(19.19/56.8)\n",
    "# R^2 = 0.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating above in sci-kit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.6620052929422553\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = [[6], [8], [10], [14],[18]]\n",
    "y = [[7], [9], [13], [17.5], [18]]\n",
    "X_test = [[8], [9],[11], [16], [12]]\n",
    "y_test = [[11], [8.5], [15], [18], [11]]\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print('R-squared:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression (multiple explanatory variable, single response variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "x1_train = [6,8,10,14,18]\n",
    "x2_train = [2,1,0,2,0]\n",
    "y_train = [7,9,13,17.5,18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset \n",
    "x1_test = [8,9,11,16,12]\n",
    "x2_test = [2,0,2,2,0]\n",
    "y_test = [11,8.5,15,18,11]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Formally, multiple linear regression is the following model:\n",
    "                Y = α + β1x1 + β2x2 + ⋅⋅⋅ + βnxn\n",
    "Where simple linear regression uses a single explanatory variable with a single coefficient, multiple linear regression uses a coefficient for each of an arbitrary number of explanatory variables. In general:\n",
    "                Y = X β    \n",
    "To find the values of β , which minimize the cost function:\n",
    "                β = (( X^T.X )^(-1)). (X^T.Y)\n",
    "\n",
    "Where, X(combination is X1 and X2) and Y(combination of Y1 and Y2) from training dataset:\n",
    "X = [[1, 6, 2], [1, 8, 1], [1, 10, 0], [1, 14, 2], [1, 18, 0]]\n",
    "Y = [7,9,13,17.5,18]\n",
    "\n",
    "NOTE: β is calculated on training data.                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can solve β using NumPy, as follows:\n",
    "from numpy.linalg import inv\n",
    "from numpy import dot, transpose\n",
    "\n",
    "X = [[1, 6, 2], [1, 8, 1], [1, 10, 0], [1, 14, 2], [1, 18, 0]]\n",
    "y = [[7], [9], [13], [17.5], [18]]\n",
    "\n",
    "beta = dot(inv(dot(transpose(X), X)), dot(transpose(X), y))\n",
    "#print(\"beta: \", beta)\n",
    "#beta.shape # beta is 3x1 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-variable Linear regression using scikit learn model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = [[6, 2], [8, 1], [10, 0], [14, 2], [18, 0]]\n",
    "y = [[7],[9],[13],[17.5], [18]]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "X_test = [[8, 2], [9, 0], [11, 2], [16, 2], [12, 0]]\n",
    "y_test = [[11],[8.5], [15],[18],[11]]\n",
    "predictions = model.predict(X_test)\n",
    "#for i, prediction in enumerate(predictions):\n",
    "#    print (\"predicted: %d target: %d \" %(prediction[0], y_test[i][0]))\n",
    "\n",
    "#Evaluate model performance\n",
    "#print ('R-squared: ', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The commutative property of (matrix) multiplication is not always satisfied in the world of linear algebra;\n",
    "the general case is that it is not satisfied. In other words, in general, Xβ≠βX. Furthermore, such operation\n",
    "may not even be permitted, since in matrix algebra, matrices must be conformable for multiplication if they\n",
    "are to be multiplied.\n",
    "\n",
    "If X is nxm matrix and β is mx1 vector then notation will be like this; without transpose: \n",
    "y=Xβ+ϵ\n",
    "\n",
    "If both X and β are column vectors of dimension (n×1) then perform transform of X.\n",
    "y=β.X^T+ϵ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
